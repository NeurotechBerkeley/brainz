{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Metrics in fMRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Import statements and load data\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import io\n",
    "from scipy import special as special\n",
    "import scipy.stats as stats\n",
    "from itertools import chain,combinations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Loading Data\"\"\"\n",
    "dataC = scipy.io.loadmat(\"data/controls.mat\")['controls'] #17, 116, 116\n",
    "dataD = scipy.io.loadmat(\"data/depressed.mat\")['depressed'] #16, 116, 116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's visualize these first\n",
    "C_avgd = np.mean(dataC, 0)   #collapse across subjects\n",
    "D_avgd = np.mean(dataD, 0)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(6.5, 3), sharey=True)\n",
    "\n",
    "axs[0].imshow(C_avgd, interpolation='nearest')\n",
    "axs[0].set_title('Control')\n",
    "#plt.show()\n",
    "axs[1].set_title('Depressed')\n",
    "axs[1].imshow(D_avgd, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric 1 - Characteristic Path Length\n",
    "\n",
    "The *characteristic path length* is the average of the shortest distances between all vertices. We'll take all pairs of vertices, calculate the shortest distance, then average the collection that we obtain. Since correlation is an undirected connectivity measure (i.e. *Corr(x, y) = Corr(y, x)*), we can cut the number of distances we have to calculate by half. \n",
    "\n",
    "A network with a low CPL (usually) has a nice situation going with information transfer (distance is minimized between nodes). For correlation values, we'll have to take absolute values in order to make sure we're measuring a true \"distance\". So our pipeline looks like:\n",
    "\n",
    "Correlation Matrices → Absolute Correlation → Distance Matrices → Characteristic Path Length\n",
    "\n",
    "Food for thought: Can an excessively low CPL be bad? In a brain, what anatomical and information-theoretic considerations might counterbalance the benefits of low CPL? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate characteristic path length\n",
    "\n",
    "import scipy.sparse.csgraph as graphs\n",
    "\n",
    "CPL_C = np.zeros(dataC.shape[0])\n",
    "CPL_D = np.zeros(dataD.shape[0])\n",
    "\n",
    "for subj in range(0, dataC.shape[0]):\n",
    "    dist = graphs.shortest_path(np.abs(dataC[subj,:,:]), method = 'D')  #our connectivity measure can be negative\n",
    "    CPL_C[subj] = np.nanmean(np.nanmean(np.where(dist!=0, dist, np.nan),1))\n",
    "    \n",
    "for subj in range(0, dataD.shape[0]):\n",
    "    dist = graphs.shortest_path(np.abs(dataD[subj,:,:]), method = 'D')  \n",
    "    CPL_D[subj] = np.nanmean(np.nanmean(np.where(dist!=0, dist, np.nan),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize our CPL measures between groups\n",
    "plt.plot(CPL_D, len(CPL_D) * [0.5], \"x\", label = 'Controls')\n",
    "plt.plot(CPL_C, len(CPL_C) * [1], \"+\", label = 'Depressed')\n",
    "plt.axis([0, 1,0,2])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# We'll test the group difference significance later..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric 2 - Connected Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Sometimes we may want to understand how many components the network can cleanly separate into (Ashton will cover the a quantification of \"cleanly\" later). In order to do this with our current data, we'll have to *threshold* our matrix by setting equal to 0 any values below a given threshold.\n",
    "\n",
    "Some things to note about this process:\n",
    "   * We assume functional connections weaker than our threshold are noise. \n",
    "   * Thresholding can make the data easier to work with or visualize.\n",
    "   * Many analyses have pointed out that network structure can vary significantly under different thresholds.\n",
    "\n",
    "\n",
    "Food for thought: What might be a principled way to choose a threshold? What network measures are robust to thresholding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connected Components: Thresholding\n",
    "\n",
    "bin_C = np.ones(dataC.shape)   # we'll have to store our adjacency matrices following binarization\n",
    "bin_D = np.ones(dataD.shape)\n",
    "\n",
    "thresh = 1.4                # our threshold; all values < epsilon will be assigned a 0\n",
    "                             # abs(values) range from 0 to 2.9; mess with epsilon accordingly \n",
    "\n",
    "sub_threshold_indices = np.abs(dataC) < thresh\n",
    "bin_C[sub_threshold_indices] = 0  \n",
    "\n",
    "sub_threshold_indices = np.abs(dataD) < thresh\n",
    "bin_D[sub_threshold_indices] = 0    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've thresholded our matrix, we can proceed with two approaches for computing connected components: the *depth-first search* algorithm and the *Laplacian*. The former is far more numerically stable and has a lower runtime but, for ~culture~, we'll also discuss the Laplacian. \n",
    "\n",
    "**Depth First Search**\n",
    "\n",
    "Pick a node. We'll branch out from it by exploring all connected nodes (the \"depth\" in the name is because, as soon as we hit a new node, we'll explore down that avenue instead - visualizing this, we're exploring deep down a branch as opposed to checking all neighboring nodes first). We'll explore all connected nodes and note when we've visited a node. \n",
    "\n",
    "Now pick another node. If it's been visited, we move on. If not, we add +1 to our connected component count and then perform the above procedure. \n",
    "\n",
    "**Laplacian**\n",
    "\n",
    "Refresher/dirty explanation: the *kernel* of a matrix is the set of all vectors x such that Ax = 0. The *dimension* is the minimal number of vectors required to represent all other vectors in the kernel through simple, linear combinations. \n",
    "\n",
    "Let *A* be the adjacency matrix (recall earlier slides). Let *D* be the diagonal degree matrix, where D<sub>i,i</sub> is the degree of the vertex *i*.\n",
    "\n",
    "The Laplacian matrix is defined as *L := D - A* and has a number of remarkable properties. For our interests, the size of the kernel yields the number of connected components. This procedure of studying a graph by using techniques from linear algebra on associated matrices is a fun/painful area called spectral graph theory. \n",
    "\n",
    "Ignoring abstract nonsense, we can find the dimension of the kernel using some fancy linear algebra. Let's compute connected components using both approaches and see if they differ. Then we'll follow up and graph both populations for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connected Components: Computation \n",
    "\n",
    "comp_C = np.zeros([dataC.shape[0], 2])  #column 1: DFS result; column 2: Laplacian result \n",
    "comp_D = np.zeros([dataD.shape[0], 2])\n",
    "\n",
    "eps = 1e-7                              #numerical stability stuff \n",
    "\n",
    "for subj in range(0, dataC.shape[0]):\n",
    "    comp_C[subj,0] = graphs.connected_components(bin_C[subj,:,:])[0]   #DFS\n",
    "    \n",
    "    lap       = graphs.laplacian(bin_C[subj,:,:])                      #Create Laplacian\n",
    "    u, s, vh  = np.linalg.svd(lap)                                     #under the hood: find nullspace\n",
    "    null_mask = (s <= eps)\n",
    "    null_space     = np.compress(null_mask, vh, axis=0)\n",
    "    comp_C[subj,1] = null_space.shape[0]\n",
    "    \n",
    "    \n",
    "for subj in range(0, dataD.shape[0]):\n",
    "    comp_D[subj,0] = graphs.connected_components(bin_D[subj,:,:])[0]\n",
    "    \n",
    "    lap = graphs.laplacian(bin_D[subj,:,:])\n",
    "    u, s, vh  = np.linalg.svd(lap)\n",
    "    null_mask = (s <= eps)\n",
    "    null_space     = np.compress(null_mask, vh, axis=0)\n",
    "    comp_D[subj,1] = null_space.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's check out our results\n",
    "\n",
    "#all values positive --> if 0 then both methods agree 100%\n",
    "print('Sum of method differences for controls is:', np.mean(np.abs(comp_D[:,0] - comp_D[:,1]))) \n",
    "print('Sum of method differences for depressed is:', np.mean(np.abs(comp_C[:,0] - comp_C[:,1])))\n",
    "\n",
    "# now plot group differences\n",
    "plt.plot(comp_C[:,0], len(comp_C[:,0]) * [0.5], \"x\", label = 'Controls')\n",
    "plt.plot(comp_D[:,0], len(comp_D[:,0]) * [1], \"+\", label = 'Depressed')\n",
    "plt.axis([0, 116,0,2])\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Robust against threshold choice? Generate 100 thresholds, look at avg connected component number for each\n",
    "\n",
    "bin_C = np.ones(dataC.shape)   \n",
    "bin_D = np.ones(dataD.shape)\n",
    "\n",
    "ind_comp_C = np.zeros(dataC.shape[0])\n",
    "ind_comp_D = np.zeros(dataC.shape[0])   \n",
    "avg_comp   = np.zeros([100, 2])            # column 1: controls; Column 2: depressed\n",
    "\n",
    "thresh = 0.025                             # initial threshold; we'll move it in increments\n",
    "\n",
    "for i in range(0, 100): \n",
    "    sub_threshold_indices_C = np.abs(dataC) < (thresh * i)  \n",
    "    sub_threshold_indices_D = np.abs(dataD) < (thresh * i)\n",
    "    bin_C[sub_threshold_indices_C] = 0  \n",
    "    bin_D[sub_threshold_indices_D] = 0 \n",
    "    \n",
    "    for subj in range(0, dataC.shape[0]):\n",
    "        ind_comp_C[subj] = graphs.connected_components(bin_C[subj,:,:])[0] \n",
    "    for subj in range(0, dataD.shape[0]):\n",
    "        ind_comp_D[subj] = graphs.connected_components(bin_D[subj,:,:])[0] \n",
    "    avg_comp[i,:] = [np.mean(ind_comp_C), np.mean(ind_comp_D)]\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0.025, 2.5, 100), avg_comp[:,0])\n",
    "plt.plot(np.linspace(0.025, 2.5, 100), avg_comp[:,1])\n",
    "plt.ylabel('# Connected Components')\n",
    "plt.xlabel('Threshold Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the number of connected components varies dramatically as a function of our threshold. This often makes analyses based off of thresholding A) harder to replicate and B) more prone to somehow getting nice results (:thinking emoji:) by introducing more degrees of freedom into experimental design.\n",
    "\n",
    "Thankfully, there are a couple approaches that can be taken to mitigate the issue here:\n",
    "   * Introduce standards.\n",
    "   * Consider how the graph structure changes as a function of the threshold. Examine graph properties at *all* possible thresholds and study the persistence of various structures as we change our \"lens\". This approach is called persistent homology and is another cool/painful area for those interested.\n",
    "   * Use another metric (Ashton, you're up)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric 3 - Modularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, modularity quantitatively measures how much we can divide a graph into modules/groups/clusters. It holds a vlaue between -1 and 1, in which a large value (1) means that the graph is densely connected within modules, and sparsely connected between modules. A low value (-1) means that entire graph highly connected, and we cannot really split it into groups (connected components).\n",
    "\n",
    "More formally, We want to subdivide the network into\n",
    "nonoverlapping groups of nodes in a way that maximizes the number of\n",
    "within-group edges, and minimizes the number of between-group edges.\n",
    "The modularity is a statistic that quantifies the degree to which the\n",
    "network may be subdivided into such clearly delineated groups.\n",
    "\n",
    "The Louvain algorithm is a fast and accurate community detection\n",
    "algorithm:\n",
    "1. Assign each node to its own community i.\n",
    "2. Calculate modularity if remove node from its community i\n",
    "3. Calculate modularity if add node to every neighboring community j\n",
    "4. Put node into community that results in highest modularity increase. If no increase possible, node stays in its own community i.\n",
    "5. Apply Steps 1-4 for all nodes in graph repeatedly, until no more modularity increase possible.\n",
    "6. Merge all nodes in one community into a new node. Incoming edge weights are summed.\n",
    "7. Repeat Steps 1-6 with graph of new nodes. \n",
    "\n",
    "The implementation of this algorithm (modularity_louvain_und_sign) can be found in modularity.py, as well as several other algorithms related to modularity. Given a connection matrix, this particular implementation returns the grouped members for each module, as well as the modularity of the network. Here we are only interested in the modularity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from modularity import modularity_louvain_und_sign\n",
    "#loop over each individual's data, and put modularity value into a list\n",
    "modularity_C = [modularity_louvain_und_sign(dataC[i])[1] for i in range(len(dataC))]\n",
    "modularity_D = [modularity_louvain_und_sign(dataD[i])[1] for i in range(len(dataD))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize our modularity measures between groups\n",
    "plt.plot(modularity_D, len(modularity_D) * [0.5], \"x\", label = 'Controls')\n",
    "plt.plot(modularity_C, len(modularity_C) * [1], \"+\", label = 'Depressed')\n",
    "plt.axis([0, 0.25, 0,2])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#Note: the y-axis here does not mean anything. \n",
    "#It is merely introduced to separate out and visualize the controls and depressed modularity data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of modularities for controls, and a list of modularities for depressed individuals, how do we know whether they are different? Can we just compare the average modularities? We will see how to test for significant difference in the statistics section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing in fMRI data\n",
    "\n",
    "#### Goal : \n",
    "\n",
    "The nobel prize winning physicist once aptly stated that \"rigor and clarity are not synonymous\". Keeping this in mind the intended emphasis of this notebook is clarity. It *is not* our goal to give a rigorous mathematical presentation of various statistical tests and methods that can be used to test connectivity levels between different brain types;however,it *is* our goal to provide clarity on how one could test whether or not there is a \"significant difference\" (used here in a very colloqiual sense) between the depressed brains and the not depressed brains. That is not to say the former is not important. We strongly reccomend taking a rigorous class in mathematical statisitcs such as Stat 135. However we choose not to delve into the mathematics here because we feel it is first important to understand ,in a very concrete way, *why* taking such a class could be interesting and useful.\n",
    "\n",
    "#### Abstraction: \n",
    "\n",
    "Often times when trying to solve a particular problem with data IRL it is important to be able to abstract away the problem from the particular to the general. Case and point: If we were the first researchers studying links between connectivity of the brain and depression, googling: \"How to tell the difference between depressed brains and not depressed brain\" would likely not be very helpful. We are on the cutting edge! If google knew the answer to this question then our funding would likely be in jeopardy. However if we were to abstract the problem away from specific, and enter a query such as: \"statistical test to determine whether or not 2 different samples were pulled from the same distribution\" then ... we would be in buisness. However knowing to enter such a query requires a certain amount of understanding; in particular  knowing to enter such a query requires us to have the ability to **think probabalistically**. \n",
    "\n",
    "In short, the main goal of this notebook is to provide a concrete example of how to think probabilistically and computationally to solve a real-world problem. In particular we want to provide the reader with an example of how we can model data with random variables and then draw inferences from that model.\n",
    "\n",
    "Part of thinking \"computationally\"(at least in the context of dealing with data) involves the ability to use computers to get an intuition about the structure of the data. A big part of this intuition comes in the form of visualization. Thus we sill start our analysis by visualizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Visualize data \n",
    "depressed = np.random.uniform(2,10,16)\n",
    "happy = np.random.uniform(5,15,17)\n",
    "\n",
    "plt.plot(depressed, len(depressed) * [0.5], \"x\")\n",
    "plt.plot(happy, len(happy) * [1.5], \"+\")\n",
    "plt.axis([0, 20,0,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model:\n",
    "\n",
    "Visually we are able to tell that there is definitiely a difference between the two groups. However if we want to try and make this difference a bit more precise. This is where knowing how to think probabalistically becomes very important.\n",
    "\n",
    "Our given data is the conectivity statistics for $n$ brains, which we will denote $B_1, \\ldots, B_n$. Wherein there is some set of indeces $D$ corresponding to depressed brains, and some set of indeces $H$ corresponding to not depressed (happy) brains. What we want to test is whether or not the connectivity of the depressed brains and happy brains are drawn from the same distribution i.e. $B_1,\\ldots,B_n \\sim \\mathbb{B}$ or if they are drawn from fundamentally different distributions i.e. $B_i \\sim \\mathbb{D} \\ \\ \\forall i \\in D$ and $B_i \\sim \\mathbb{H} \\ \\ \\forall i \\in H$.\n",
    "\n",
    "#### TODO : Talk about distributions (in case they don't know) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "At this point I really want to emphasize that everything shown above is THE MOST IMPORTANT PART OF THIS NOTEBOOK. Because IRL now would be the time to start some frantic googling: http://bfy.tw/DylF to try and figure out a good way to test the differences in brain. If you are able to do what I have described above, namely think probabilistically and computationally you have picked up the main point of this portion of the presentation. That is not to say that the following portion of the notebook is not important. It is just the analysis that follows presents an outline of some of the take home messages, one would have gleamed after a lot of time thinking about the problem. In other words DO NOT FEEL BAD IF YOU ARE NOT ABLE TO FOLLOW THE NEXT SESSION. Especially if you are a biologist since the folowing presents a certain thought process that takes a lot of time to develop -- time that you may not have put in if you haven't taken a lot of more theoretical classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/math_ahead.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point one idea is to try and model the conditional probability that we observe the current variation in brains given they were drawn from different distributions i.e.\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(B_1,\\ldots, B_n | B_i \\sim \\mathbb{D} \\ \\ \\forall i \\in D, B_i \\sim \\mathbb{H} \\ \\ \\forall i \\in H )\n",
    "$$\n",
    "\n",
    "The idea being if this probability is high then we have strong evidence suggesting there is a difference in connectivity levels of depressed and happy brains. However using such a model requires prior knowledge of the distributions $\\mathbb{D}$ and $\\mathbb{H}$. Knowledge that we do not neccessarily have *a priori*. Keeping this in mind let us consider the second natural approach to try and guage whether or not there is a difference between happy and depressed brains. Namely:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(B_1,\\ldots, B_n | B_i \\sim \\mathbb{B} \\ \\ \\forall i ).\n",
    "$$\n",
    "\n",
    "This approach seems to be superior to the previous one in the sense that it only deals with one unknown distribution rather than $2$. Furthermore with this framing of the problem we should be able to rank all of the brains from most connected to least connected -- namely $R(B_1), ...,R(B_i)$ and calculate the probability the rankings appear the way they do assuming $B_i \\sim \\mathbb{B} \\ \\ \\forall i$. More rigorously we will look at\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(R(B_1),\\ldots, R(B_n) | B_i \\sim \\mathbb{B} \\ \\ \\forall i ).\n",
    "$$\n",
    "\n",
    "where $R$ is a function that maps the test statistic to its ranking. This seems to have promise since once we look at the rankings we are somehow overcoming the problem of the unknown distribution. However we are not quite there. In particular in it's hard to see how this calculation bears any practical significance given as n grows very large the probablity that the rankings take any specific form tends towards $0$. Thus all the probabilities are low. One response is to consider the rank-sum of all of the depressed brains and calculate what the probability of seeing a rank-sum as extreme or more extreme then the computed rank-sum. More precisely\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\sum_{i \\in D} R(B_i) \\ | \\ B_i \\sim \\mathbb{B} \\ \\ \\forall i ).\n",
    "$$\n",
    "\n",
    "Here we are measuring the probability that a randomly selected subset of size $|D|$ from an original set of size $|B|$ has rank sum as extrme as $sum_{i \\in D} R(B_i)$. Indeed, we can calculate the distribution of the rank sum of a subset of size $|D|$ from a superset of size $|S|$ from first principles and calculate $ \\mathbb{P}(\\sum_{i \\in D} R(B_i) \\ | \\ B_i \\sim \\mathbb{B} \\ \\ \\forall i )$. This is in fact called the Wilcoxon Rank sum test; and there is code for it in python !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Wilcoxon rank sum test\n",
    "# Reference https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.wilcoxon.html\n",
    "test_statistic,p_value= stats.ranksums(depressed,happy)\n",
    "print(\"-------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"The probability we observe a rank sum this extreme given the brains were drawn from the same distribution is:\")\n",
    "print(p_value)\n",
    "print(\"-------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"In other words if the brains were drawn from the same distribution we would expect to see a rank sum this extreme\")\n",
    "print(str(p_value*100) + \"% of the time\")\n",
    "print(\"-------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Concept Check \n",
    "\n",
    "In the real world we would just, google it as is done above. But for pedagogical reasons lets see if we can code this up in python. \n",
    "\n",
    "Remark: Below I use an iterator (thats what combinations returns).  An iterator is an object in python that CAN be iterated over; but CANNOT be\"accessed\" like a list. In python this tends to give certain performaince \n",
    "benefits. See : https://stackoverflow.com/questions/628903/performance-advantages-to-iterators\n",
    "\n",
    "If you are interested in learning more about iterators in python. You should read\n",
    "the wiki on streams: https://en.wikipedia.org/wiki/Stream_(computing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_p_val_func_for(num_small ,num_total):\n",
    "    \"\"\"\n",
    "    Returns function that evaluates p-value given sum_statisitc\n",
    "    from num_small,num_total distribution\n",
    "    \"\"\"\n",
    "    num_sums = int(round(special.binom(33,16)))\n",
    "    vals = np.zeros(num_sums)\n",
    "    i = 0\n",
    "    for combination in combinations(np.arange(num_total),num_small):\n",
    "        vals[i] = sum(combination)\n",
    "    vals = np.sort(vals)\n",
    "    def compute_p_value(obs_sum):\n",
    "        num_bigger_or_equal_to = 0\n",
    "        for val in vals:\n",
    "            if val > obs_sum:\n",
    "                num_bigger_or_equal_to += 1\n",
    "            else:\n",
    "                break\n",
    "        prop_bigger = num_bigger_or_equal_to / len(vals)\n",
    "        p_value = 2 * min(prop_bigger,1-prop_bigger)\n",
    "        return p_value         \n",
    "    return comput_p_value\n",
    "\n",
    "def compute_rank_sum_stat(depressed,happy):\n",
    "    \"\"\"\n",
    "    Part 0 Compute CDF for given sample sizes (Assume already have)\n",
    "    Part 1 get rank sum statistic for depressed\n",
    "    Part 2 Use CDF to Tell if rank sum computed is \"statistically significant\"\n",
    "    \"\"\"\n",
    "    concatenated = np.concatenate((depressed, happy), axis=0)\n",
    "    sorted_lst = np.sort(concatenated)\n",
    "    def compute_rank_of(x):\n",
    "        ix = np.isin(sorted_lst,x)\n",
    "        loc = np.where(ix)\n",
    "        return loc\n",
    "    return sum([compute_rank_of(depressed_brain) for depressed_brain in depressed])\n",
    "\n",
    "def comput_p_value(depressed,happy):\n",
    "    p_val = get_p_val_func_for(len(depressed),len(happy))\n",
    "    obs_sum = compute_rank_sum_stat(depressed,happy)\n",
    "    return p_val(obs_sum)\n",
    "\n",
    "\n",
    "p_val_func = get_p_val_func_for(3,5) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_val_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Whew! We did it! [Natural question how do we do this for large sample sizes?] \n",
    "\n",
    "[Put picture of asymptotic results asymptopical Normal distribution]\n",
    "\n",
    "If you know what the two sample non-parametric bootstrap you may find this post interesting \n",
    "https://stats.stackexchange.com/questions/61787/can-bootstrap-be-used-to-replace-non-parametric-tests?rq=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you are interested here are some additional articles/sources\n",
    "\n",
    "https://www.scientificamerican.com/article/brain-imaging-identifies-different-types-of-depression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
