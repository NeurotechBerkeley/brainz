{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Import statements and load data\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import io\n",
    "from scipy import special as special\n",
    "import scipy.stats as stats\n",
    "from itertools import chain,combinations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Loading Data\"\"\"\n",
    "controls = scipy.io.loadmat(\"data/controls.mat\")['controls'] #17, 116, 116\n",
    "depressed = scipy.io.loadmat(\"data/depressed.mat\")['depressed'] #16, 116, 116"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Metrics in fMRI data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric 1 - Modularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal community structure is a subdivision of the network into\n",
    "nonoverlapping groups of nodes in a way that maximizes the number of\n",
    "within-group edges, and minimizes the number of between-group edges.\n",
    "The modularity is a statistic that quantifies the degree to which the\n",
    "network may be subdivided into such clearly delineated groups.\n",
    "\n",
    "The Louvain algorithm is a fast and accurate community detection\n",
    "algorithm (at the time of writing):\n",
    "1. Assign each node to its own community i.\n",
    "2. Calculate Q if remove node from its community i\n",
    "3. Calculate Q if add node to every neighboring community j\n",
    "4. Put node into community that results in highest Q increase. If no increase possible, node stays in its own community i.\n",
    "5. Apply Steps 1-4 for all nodes in graph repeatedly, until no more Q increase possible.\n",
    "6. Merge all nodes in one community into a new node. Incoming edge weights are summed.\n",
    "7. Repeat Steps 1-6 with graph of new nodes. \n",
    "\n",
    "The implementation of this algorithm (modularity_louvain_und_sign) can be found in modularity.py, as well as several other algorithms related to modularity. Given a connection matrix, this particular implementation returns the grouped members for each module, as well as the modularity of the network. Here we are only interested in the modularity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from modularity import modularity_louvain_und_sign\n",
    "#loop over each individual's data, and put modularity value into a list\n",
    "controls_modularity = [modularity_louvain_und_sign(controls[i])[1] for i in range(len(controls))]\n",
    "depressed_modularity = [modularity_louvain_und_sign(depressed[i])[1] for i in range(len(depressed))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modularities for controls:  [0.059443680818228331, 0.068157281381724111, 0.036696603441131043, 0.046041445064078305, 0.04818120191563325, 0.075665084821964768, 0.043004237544202503, 0.072371828446451306, 0.061087761643920865, 0.050232720453102571, 0.1049458175864072, 0.064073425712800128, 0.056058346922762098, 0.071353344191112111, 0.077269903160571474, 0.045738446346565008, 0.066551116087541121]\n",
      "---\n",
      "modularities for depressed:  [0.093798528040905216, 0.20874386153442956, 0.12986902599520722, 0.07149584611675637, 0.12247370674490467, 0.06125322570416606, 0.097108365704986405, 0.065300239894596213, 0.023415746879014133, 0.066976354165625654, 0.097791794975514851, 0.10527739407118321, 0.068366668314025172, 0.1308631546267856, 0.039603224394632201, 0.081486142423716795]\n"
     ]
    }
   ],
   "source": [
    "print(\"modularities for controls: \", controls_modularity)\n",
    "print(\"---\")\n",
    "print(\"modularities for depressed: \", depressed_modularity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of modularities for controls, and a list of modularities for depressed individuals, how do we know whether they are different? Can we just compare the average modularities? We will see how to test for significant difference in the statistics section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing in fMRI data\n",
    "\n",
    "#### Goal : \n",
    "\n",
    "The nobel prize winning physicist once aptly stated that \"rigor and clarity are not synonymous\". Keeping this in mind the intended emphasis of this notebook is clarity. It *is not* our goal to give a rigorous mathematical presentation of various statistical tests and methods that can be used to test connectivity levels between different brain types;however,it *is* our goal to provide clarity on how one could test whether or not there is a \"significant difference\" (used here in a very colloqiual sense) between the depressed brains and the not depressed brains. That is not to say the former is not important. We strongly reccomend taking a rigorous class in mathematical statisitcs such as Stat 135. However we choose not to delve into the mathematics here because we feel it is first important to understand ,in a very concrete way, *why* taking such a class could be interesting and useful.\n",
    "\n",
    "#### Abstraction: \n",
    "\n",
    "Often times when trying to solve a particular problem with data IRL it is important to be able to abstract away the problem from the particular to the general. Case and point: If we were the first researchers studying links between connectivity of the brain and depression, googling: \"How to tell the difference between depressed brains and not depressed brain\" would likely not be very helpful. We are on the cutting edge! If google knew the answer to this question then our funding would likely be in jeopardy. However if we were to abstract the problem away from specific, and enter a query such as: \"statistical test to determine whether or not 2 different samples were pulled from the same distribution\" then ... we would be in buisness. However knowing to enter such a query requires a certain amount of understanding; in particular  knowing to enter such a query requires us to have the ability to **think probabalistically**. \n",
    "\n",
    "In short, the main goal of this notebook is to provide a concrete example of how to think probabilistically and computationally to solve a real-world problem. In particular we want to provide the reader with an example of how we can model data with random variables and then draw inferences from that model.\n",
    "\n",
    "Part of thinking \"computationally\"(at least in the context of dealing with data) involves the ability to use computers to get an intuition about the structure of the data. A big part of this intuition comes in the form of visualization. Thus we sill start our analysis by visualizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Visualize data \n",
    "depressed = np.random.uniform(2,10,16)\n",
    "happy = np.random.uniform(5,15,17)\n",
    "\n",
    "plt.plot(depressed, len(depressed) * [0.5], \"x\")\n",
    "plt.plot(happy, len(happy) * [1.5], \"+\")\n",
    "plt.axis([0, 20,0,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model:\n",
    "\n",
    "Visually we are able to tell that there is definitiely a difference between the two groups. However if we want to try and make this difference a bit more precise. This is where knowing how to think probabalistically becomes very important.\n",
    "\n",
    "Our given data is the conectivity statistics for $n$ brains, which we will denote $B_1, \\ldots, B_n$. Wherein there is some set of indeces $D$ corresponding to depressed brains, and some set of indeces $H$ corresponding to not depressed (happy) brains. What we want to test is whether or not the connectivity of the depressed brains and happy brains are drawn from the same distribution i.e. $B_1,\\ldots,B_n \\sim \\mathbb{B}$ or if they are drawn from fundamentally different distributions i.e. $B_i \\sim \\mathbb{D} \\ \\ \\forall i \\in D$ and $B_i \\sim \\mathbb{H} \\ \\ \\forall i \\in H$.\n",
    "\n",
    "#### TODO : Talk about distributions (in case they don't know) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "At this point I really want to emphasize that everything shown above is THE MOST IMPORTANT PART OF THIS NOTEBOOK. Because IRL now would be the time to start some frantic googling: http://bfy.tw/DylF to try and figure out a good way to test the differences in brain. If you are able to do what I have described above, namely think probabilistically and computationally you have picked up the main point of this portion of the presentation. That is not to say that the following portion of the notebook is not important. It is just the analysis that follows presents an outline of some of the take home messages, one would have gleamed after a lot of time thinking about the problem. In other words DO NOT FEEL BAD IF YOU ARE NOT ABLE TO FOLLOW THE NEXT SESSION. Especially if you are a biologist since the folowing presents a certain thought process that takes a lot of time to develop -- time that you may not have put in if you haven't taken a lot of more theoretical classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/math_ahead.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point one idea is to try and model the conditional probability that we observe the current variation in brains given they were drawn from different distributions i.e.\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(B_1,\\ldots, B_n | B_i \\sim \\mathbb{D} \\ \\ \\forall i \\in D, B_i \\sim \\mathbb{H} \\ \\ \\forall i \\in H )\n",
    "$$\n",
    "\n",
    "The idea being if this probability is high then we have strong evidence suggesting there is a difference in connectivity levels of depressed and happy brains. However using such a model requires prior knowledge of the distributions $\\mathbb{D}$ and $\\mathbb{H}$. Knowledge that we do not neccessarily have *a priori*. Keeping this in mind let us consider the second natural approach to try and guage whether or not there is a difference between happy and depressed brains. Namely:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(B_1,\\ldots, B_n | B_i \\sim \\mathbb{B} \\ \\ \\forall i ).\n",
    "$$\n",
    "\n",
    "This approach seems to be superior to the previous one in the sense that it only deals with one unknown distribution rather than $2$. Furthermore with this framing of the problem we should be able to rank all of the brains from most connected to least connected -- namely $R(B_1), ...,R(B_i)$ and calculate the probability the rankings appear the way they do assuming $B_i \\sim \\mathbb{B} \\ \\ \\forall i$. More rigorously we will look at\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(R(B_1),\\ldots, R(B_n) | B_i \\sim \\mathbb{B} \\ \\ \\forall i ).\n",
    "$$\n",
    "\n",
    "where $R$ is a function that maps the test statistic to its ranking. This seems to have promise since once we look at the rankings we are somehow overcoming the problem of the unknown distribution. However we are not quite there. In particular in it's hard to see how this calculation bears any practical significance given as n grows very large the probablity that the rankings take any specific form tends towards $0$. Thus all the probabilities are low. One response is to consider the rank-sum of all of the depressed brains and calculate what the probability of seeing a rank-sum as extreme or more extreme then the computed rank-sum. More precisely\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\sum_{i \\in D} R(B_i) \\ | \\ B_i \\sim \\mathbb{B} \\ \\ \\forall i ).\n",
    "$$\n",
    "\n",
    "Here we are measuring the probability that a randomly selected subset of size $|D|$ from an original set of size $|B|$ has rank sum as extrme as $sum_{i \\in D} R(B_i)$. Indeed, we can calculate the distribution of the rank sum of a subset of size $|D|$ from a superset of size $|S|$ from first principles and calculate $ \\mathbb{P}(\\sum_{i \\in D} R(B_i) \\ | \\ B_i \\sim \\mathbb{B} \\ \\ \\forall i )$. This is in fact called the Wilcoxon Rank sum test; and there is code for it in python !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Wilcoxon rank sum test\n",
    "# Reference https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.wilcoxon.html\n",
    "test_statistic,p_value= stats.ranksums(depressed,happy)\n",
    "print(\"-------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"The probability we observe a rank sum this extreme given the brains were drawn from the same distribution is:\")\n",
    "print(p_value)\n",
    "print(\"-------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"In other words if the brains were drawn from the same distribution we would expect to see a rank sum this extreme\")\n",
    "print(str(p_value*100) + \"% of the time\")\n",
    "print(\"-------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Concept Check \n",
    "\n",
    "In the real world we would just, google it as is done above. But for pedagogical reasons lets see if we can code this up in python. \n",
    "\n",
    "Remark: Below I use an iterator (thats what combinations returns).  An iterator is an object in python that CAN be iterated over; but CANNOT be\"accessed\" like a list. In python this tends to give certain performaince \n",
    "benefits. See : https://stackoverflow.com/questions/628903/performance-advantages-to-iterators\n",
    "\n",
    "If you are interested in learning more about iterators in python. You should read\n",
    "the wiki on streams: https://en.wikipedia.org/wiki/Stream_(computing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_p_val_func_for(num_small ,num_total):\n",
    "    \"\"\"\n",
    "    Returns function that evaluates p-value given sum_statisitc\n",
    "    from num_small,num_total distribution\n",
    "    \"\"\"\n",
    "    num_sums = int(round(special.binom(33,16)))\n",
    "    vals = np.zeros(num_sums)\n",
    "    i = 0\n",
    "    for combination in combinations(np.arange(num_total),num_small):\n",
    "        vals[i] = sum(combination)\n",
    "    vals = np.sort(vals)\n",
    "    def compute_p_value(obs_sum):\n",
    "        num_bigger_or_equal_to = 0\n",
    "        for val in vals:\n",
    "            if val > obs_sum:\n",
    "                num_bigger_or_equal_to += 1\n",
    "            else:\n",
    "                break\n",
    "        prop_bigger = num_bigger_or_equal_to / len(vals)\n",
    "        p_value = 2 * min(prop_bigger,1-prop_bigger)\n",
    "        return p_value         \n",
    "    return comput_p_value\n",
    "\n",
    "def compute_rank_sum_stat(depressed,happy):\n",
    "    \"\"\"\n",
    "    Part 0 Compute CDF for given sample sizes (Assume already have)\n",
    "    Part 1 get rank sum statistic for depressed\n",
    "    Part 2 Use CDF to Tell if rank sum computed is \"statistically significant\"\n",
    "    \"\"\"\n",
    "    concatenated = np.concatenate((depressed, happy), axis=0)\n",
    "    sorted_lst = np.sort(concatenated)\n",
    "    def compute_rank_of(x):\n",
    "        ix = np.isin(sorted_lst,x)\n",
    "        loc = np.where(ix)\n",
    "        return loc\n",
    "    return sum([compute_rank_of(depressed_brain) for depressed_brain in depressed])\n",
    "\n",
    "def comput_p_value(depressed,happy):\n",
    "    p_val = get_p_val_func_for(len(depressed),len(happy))\n",
    "    obs_sum = compute_rank_sum_stat(depressed,happy)\n",
    "    return p_val(obs_sum)\n",
    "\n",
    "\n",
    "p_val_func = get_p_val_func_for(3,5) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_val_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Whew! We did it! [Natural question how do we do this for large sample sizes?] \n",
    "\n",
    "[Put picture of asymptotic results asymptopical Normal distribution]\n",
    "\n",
    "If you know what the two sample non-parametric bootstrap you may find this post interesting \n",
    "https://stats.stackexchange.com/questions/61787/can-bootstrap-be-used-to-replace-non-parametric-tests?rq=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you are interested here are some additional articles/sources\n",
    "\n",
    "https://www.scientificamerican.com/article/brain-imaging-identifies-different-types-of-depression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
