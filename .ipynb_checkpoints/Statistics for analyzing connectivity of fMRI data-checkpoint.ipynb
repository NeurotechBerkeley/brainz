{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing in fMRI data\n",
    "\n",
    "#### Goal : \n",
    "\n",
    "The nobel prize winning physicist once aptly stated that \"rigor and clarity are not synonymous\". Keeping this in mind the intended emphasis of this notebook is clarity. It *is not* our goal to give a rigorous mathematical presentation of various statistical tests and methods that can be used to test connectivity levels between different brain types;however,it *is* our goal to provide clarity on how one could test whether or not there is a \"significant difference\" (used here in a very colloqiual sense) between the depressed brains and the not depressed brains. That is not to say the former is not important. We strongly reccomend taking a rigorous class in mathematical statisitcs such as Stat 135. However we choose not to delve into the mathematics here because we feel it is first important to understand ,in a very concrete way, *why* taking such a class could be interesting and useful.\n",
    "\n",
    "#### Abstraction: \n",
    "\n",
    "Often times when trying to solve a particular problem with data IRL it is important to be able to abstract away the problem from the particular to the general. Case and point: If we were the first researchers studying links between connectivity of the brain and depression, googling: \"How to tell the difference between depressed brains and not depressed brain\" would likely not be very helpful. We are on the cutting edge! If google knew the answer to this question then our funding would likely be in jeopardy. However if we were to abstract the problem away from specific, and enter a query such as: \"statistical test to determine whether or not 2 different samples were pulled from the same distribution\" then ... we would be in buisness. However knowing to enter such a query requires a certain amount of understanding; in particular  knowing to enter such a query requires us to have the ability to **think probabalistically**. \n",
    "\n",
    "In short, the main goal of this notebook is to provide a concrete example of how to think probabilistically and computationally to solve a real-world problem. In particular we want to provide the reader with an example of how we can model data with random variables and then draw inferences from that model.\n",
    "\n",
    "Part of thinking \"computationally\"(at least in the context of dealing with data) involves the ability to use computers to get an intuition about the structure of the data. A big part of this intuition comes in the form of visualization. Thus we sill start our analysis by visualizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "## Import statements and load data\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "from scipy import special as special\n",
    "import scipy.stats as stats\n",
    "from itertools import chain,combinations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3AEgof8HKRRmV8h1YvYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115104a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Visualize data \n",
    "depressed = np.random.uniform(2,10,16)\n",
    "happy = np.random.uniform(5,15,17)\n",
    "\n",
    "plt.plot(depressed, len(depressed) * [0.5], \"x\")\n",
    "plt.plot(happy, len(happy) * [1.5], \"+\")\n",
    "plt.axis([0, 20,0,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model:\n",
    "\n",
    "Visually we are able to tell that there is definitiely a difference between the two groups. However if we want to try and make this difference a bit more precise. This is where knowing how to think probabalistically becomes very important.\n",
    "\n",
    "Our given data is the conectivity statistics for $n$ brains, which we will denote $B_1, \\ldots, B_n$. Wherein there is some set of indeces $D$ corresponding to depressed brains, and some set of indeces $H$ corresponding to not depressed (happy) brains. What we want to test is whether or not the connectivity of the depressed brains and happy brains are drawn from the same distribution i.e. $B_1,\\ldots,B_n \\sim \\mathbb{B}$ or if they are drawn from fundamentally different distributions i.e. $B_i \\sim \\mathbb{D} \\ \\ \\forall i \\in D$ and $B_i \\sim \\mathbb{H} \\ \\ \\forall i \\in H$.\n",
    "\n",
    "#### TODO : Talk about distributions (in case they don't know) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "At this point I really want to emphasize that everything shown above is THE MOST IMPORTANT PART OF THIS NOTEBOOK. Because IRL now would be the time to start some frantic googling: http://bfy.tw/DylF to try and figure out a good way to test the differences in brain. If you are able to do what I have described above, namely think probabilistically and computationally you have picked up the main point of this portion of the presentation. That is not to say that the following portion of the notebook is not important. It is just the analysis that follows presents an outline of some of the take home messages, one would have gleamed after a lot of time thinking about the problem. In other words DO NOT FEEL BAD IF YOU ARE NOT ABLE TO FOLLOW THE NEXT SESSION. Especially if you are a biologist since the folowing presents a certain thought process that takes a lot of time to develop -- time that you may not have put in if you haven't taken a lot of more theoretical classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/math_ahead.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point one idea is to try and model the conditional probability that we observe the current variation in brains given they were drawn from different distributions i.e.\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(B_1,\\ldots, B_n | B_i \\sim \\mathbb{D} \\ \\ \\forall i \\in D, B_i \\sim \\mathbb{H} \\ \\ \\forall i \\in H )\n",
    "$$\n",
    "\n",
    "The idea being if this probability is high then we have strong evidence suggesting there is a difference in connectivity levels of depressed and happy brains. However using such a model requires prior knowledge of the distributions $\\mathbb{D}$ and $\\mathbb{H}$. Knowledge that we do not neccessarily have *a priori*. Keeping this in mind let us consider the second natural approach to try and guage whether or not there is a difference between happy and depressed brains. Namely:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(B_1,\\ldots, B_n | B_i \\sim \\mathbb{B} \\ \\ \\forall i ).\n",
    "$$\n",
    "\n",
    "This approach seems to be superior to the previous one in the sense that it only deals with one unknown distribution rather than $2$. Furthermore with this framing of the problem we should be able to rank all of the brains from most connected to least connected -- namely $R(B_1), ...,R(B_i)$ and calculate the probability the rankings appear the way they do assuming $B_i \\sim \\mathbb{B} \\ \\ \\forall i$. More rigorously we will look at\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(R(B_1),\\ldots, R(B_n) | B_i \\sim \\mathbb{B} \\ \\ \\forall i ).\n",
    "$$\n",
    "\n",
    "where $R$ is a function that maps the test statistic to its ranking. This seems to have promise since once we look at the rankings we are somehow overcoming the problem of the unknown distribution. However we are not quite there. In particular in it's hard to see how this calculation bears any practical significance given as n grows very large the probablity that the rankings take any specific form tends towards $0$. Thus all the probabilities are low. One response is to consider the rank-sum of all of the depressed brains and calculate what the probability of seeing a rank-sum as extreme or more extreme then the computed rank-sum. More precisely\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\sum_{i \\in D} R(B_i) \\ | \\ B_i \\sim \\mathbb{B} \\ \\ \\forall i ).\n",
    "$$\n",
    "\n",
    "Here we are measuring the probability that a randomly selected subset of size $|D|$ from an original set of size $|B|$ has rank sum as extrme as $sum_{i \\in D} R(B_i)$. Indeed, we can calculate the distribution of the rank sum of a subset of size $|D|$ from a superset of size $|S|$ from first principles and calculate $ \\mathbb{P}(\\sum_{i \\in D} R(B_i) \\ | \\ B_i \\sim \\mathbb{B} \\ \\ \\forall i )$. This is in fact called the Wilcoxon Rank sum test; and there is code for it in python !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------\n",
      "The probability we observe a rank sum this extreme given the brains were drawn from the same distribution is:\n",
      "0.160064575297\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "In other words if the brains were drawn from the same distribution we would expect to see a rank sum this extreme\n",
      "16.0064575297% of the time\n",
      "-------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Wilcoxon rank sum test\n",
    "# Reference https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.wilcoxon.html\n",
    "test_statistic,p_value= stats.ranksums(depressed,happy)\n",
    "print(\"-------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"The probability we observe a rank sum this extreme given the brains were drawn from the same distribution is:\")\n",
    "print(p_value)\n",
    "print(\"-------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"In other words if the brains were drawn from the same distribution we would expect to see a rank sum this extreme\")\n",
    "print(str(p_value*100) + \"% of the time\")\n",
    "print(\"-------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Extra (If Time)\n",
    "\n",
    "In the real world we would just, google it as is done above. But as fun take home problem seee if you can write out the code manually. Below I have posted some skeleton code to get you started.\n",
    "\n",
    "Remark: Below I use an iterator (thats what combinations returns).  An iterator is an object in python that CAN be iterated over; but CANNOT be\"accessed\" like a list. In python this tends to give certain performaince \n",
    "benefits. See : https://stackoverflow.com/questions/628903/performance-advantages-to-iterators\n",
    "\n",
    "If you are interested in learning more about iterators in python. You should read\n",
    "the wiki on streams: https://en.wikipedia.org/wiki/Stream_(computing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_p_val_func_for(num_small ,num_total):\n",
    "    \"\"\"\n",
    "    Returns function that evaluates p-value given sum_statisitc\n",
    "    from num_small,num_total distribution\n",
    "    \"\"\"        \n",
    "    return \n",
    "\n",
    "def get_ranking_distributions(num_small, num_total):\n",
    "    \"\"\"\n",
    "    Gets ranking distribution that we are dealing with returns it as a list\n",
    "    \"\"\"\n",
    "    num_sums = int(round(special.binom(num_total,num_small)))\n",
    "    vals = np.zeros(num_sums)\n",
    "    i = 0\n",
    "    for combination in combinations(np.arange(num_total),num_small):\n",
    "        vals[i] = sum(combination)\n",
    "        i += 1\n",
    "    vals = np.sort(vals)\n",
    "    return vals\n",
    "\n",
    "def get_p_val_func(vals):\n",
    "    \"\"\"\n",
    "    Returns function that computes the p-value of the observed sum from distribution\n",
    "    of vals\n",
    "    \"\"\"\n",
    "    return p_val_func\n",
    "\n",
    "def compute_rank_sum_stat(depressed,happy):\n",
    "    \"\"\"\n",
    "    Part 0 Compute CDF for given sample sizes (Assume already have)\n",
    "    Part 1 get rank sum statistic for depressed\n",
    "    Part 2 Use CDF to Tell if rank sum computed is \"statistically significant\"\n",
    "    \"\"\"\n",
    "    concatenated = np.concatenate((depressed, happy), axis=0)\n",
    "    sorted_lst = np.sort(concatenated)\n",
    "    ranks = np.sum([compute_rank_of(depressed_brain,sorted_lst) for depressed_brain in depressed])\n",
    "    return ranks\n",
    "\n",
    "def compute_rank_of(x,sorted_lst):\n",
    "    \"\"\"\n",
    "    Assume x lives in sorted list. This function finds its ranking.\n",
    "    \"\"\"\n",
    "    return \n",
    "\n",
    "\n",
    "def compute_p_value(depressed,happy):\n",
    "    \"\"\" \n",
    "    Compute p-value using rank Wilcoxon Rank Sum statistic to see it their is\n",
    "    a difference between the connectivity levels for depressed and happy brains \n",
    "    \"\"\"\n",
    "    return\n",
    "\n",
    "### What other tests should we use??\n",
    "assert np.allclose(get_ranking_distributions(1,3),np.array([0,1,2])) == True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you are interested here are some additional thoughts, articles, and sources\n",
    "\n",
    "Interesting question what is the run time of the above algorithm? Isn't very slow? How do we do this for large sample sizes? If you are interested look up asymptopical Normal distribution Wilcoxon Rank Sum Test.\n",
    "\n",
    "What we are interested in starts on first page of the below pdf <br>\n",
    "http://sites.stat.psu.edu/~drh20/asymp/fall2003/lectures/pages63to75.pdf\n",
    "\n",
    "If you know what the two sample non-parametric bootstrap you may find this post interesting <br>\n",
    "https://stats.stackexchange.com/questions/61787/can-bootstrap-be-used-to-replace-non-parametric-tests?rq=1\n",
    "\n",
    "If you want to know more about the science:<br>\n",
    "https://www.scientificamerican.com/article/brain-imaging-identifies-different-types-of-depression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
